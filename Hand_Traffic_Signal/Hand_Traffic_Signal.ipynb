{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Crop\n",
    "\n",
    "- Image Crop의 경우 [BaseLine](https://www.dacon.io/competitions/official/235806/codeshare/3365?page=1&dtype=recent)과 [5252님](https://www.dacon.io/competitions/official/235805/codeshare/3362?page=2&dtype=recent)의 코드를 함께 이용했습니다.\n",
    "\n",
    "5252님과 Dacon.MockingJay님 감사합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter('ignore')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/data/competition/Dacon/Traffic_Hand_Signal/dataset'\n",
    "\n",
    "train_path = data_path + '/train'\n",
    "\n",
    "test_path = data_path + '/test'\n",
    "\n",
    "action_information = pd.read_csv(data_path + '/action_information.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folders = sorted(glob(train_path + '/*'), key = lambda x : int(x.split('/file_')[-1]))\n",
    "\n",
    "test_folders  = sorted(glob(test_path + '/*'), key = lambda x : int(x.split('/file_')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image_directory = data_path + '/new_images'\n",
    "new_train_image_directory = new_image_directory + '/train'\n",
    "new_test_image_directory = new_image_directory + '/test'\n",
    "\n",
    "action_information = pd.read_csv(data_path + '/action_information.csv')\n",
    "sample_submission = pd.read_csv(data_path + '/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.get_dummies(action_information[['Label']], columns = ['Label']).to_numpy()\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_new_dir(path) : \n",
    "    if os.path.isdir(path) == False:\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_new_dir(new_image_directory)\n",
    "make_new_dir(new_train_image_directory)\n",
    "make_new_dir(new_test_image_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_directories = np.array(sorted(glob(train_path + '/*'), key = lambda x : int(x.split('/')[-1].split('_')[-1])))\n",
    "\n",
    "for train_directory in tqdm(train_directories, total = len(train_directories)) : \n",
    "    file_name = train_directory.split('/')[-1]\n",
    "    make_new_dir(new_train_image_directory + '/'+file_name)\n",
    "    \n",
    "    image_paths = sorted(glob(train_directory + '/*.jpg'), key = lambda x : int(x.split('/')[-1].replace('.jpg','')))\n",
    "    json_path   = glob(train_directory + '/*.json')[0]\n",
    "\n",
    "    js = json.load(open(json_path))\n",
    "    target = js.get('action')\n",
    "    target = classes[target]\n",
    "    bounding_boxes = js.get('sequence').get('bounding_box')\n",
    "    bounding_boxes = [(float(a),float(b),float(c),float(d)) for a,b,c,d in bounding_boxes]\n",
    "\n",
    "    for image_path, bounding_box in zip(image_paths, bounding_boxes) : \n",
    "        image = Image.open(image_path)\n",
    "        image = image.crop(bounding_box) # left top right bottom\n",
    "        image = image.resize((224,224))\n",
    "        image.save(new_train_image_directory + image_path.split('/train')[1])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_directories = np.array(sorted(glob(test_path + '/*'), key = lambda x : int(x.split('/')[-1].split('_')[-1])))\n",
    "\n",
    "for test_directory in tqdm(test_directories, total = len(test_directories)) : \n",
    "    file_name = test_directory.split('/')[-1]\n",
    "    make_new_dir(new_test_image_directory + '/'+file_name)\n",
    "    \n",
    "    image_paths = sorted(glob(test_directory + '/*.jpg'), key = lambda x : int(x.split('/')[-1].replace('.jpg','')))\n",
    "    json_path   = glob(test_directory + '/*.json')[0]\n",
    "\n",
    "    js = json.load(open(json_path))\n",
    "    target = js.get('action')\n",
    "    target = classes[target]\n",
    "    bounding_boxes = js.get('sequence').get('bounding_box')\n",
    "    bounding_boxes = [(float(a),float(b),float(c),float(d)) for a,b,c,d in bounding_boxes]\n",
    "\n",
    "    for image_path, bounding_box in zip(image_paths, bounding_boxes) : \n",
    "        image = Image.open(image_path)\n",
    "        image = image.crop(bounding_box)\n",
    "        image = image.resize((224,224))\n",
    "        image.save(new_test_image_directory + image_path.split('/test')[1])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import albumentations as A\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/data/competition/Dacon/Traffic_Hand_Signal/dataset'\n",
    "\n",
    "train_path = data_path + '/train'\n",
    "\n",
    "train_image_path = data_path + '/new_images/train'\n",
    "\n",
    "action_information = pd.read_csv(data_path + '/action_information.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folders = sorted(glob(train_path + '/*'), key = lambda x : int(x.split('/file_')[-1]))\n",
    "\n",
    "train_img_folders = sorted(glob(train_image_path + '/*'), key = lambda x : int(x.split('/file_')[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 데이터를 사용하지 않고 데이터의 일부분만을 사용하여 학습하였더니 더 잘 학습되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "\n",
    "for i in train_img_folders:\n",
    "    train_imgs = sorted(glob(i+\"/*.jpg\"), key = lambda x : int(x.split('.jpg')[0].split(\"/\")[-1]))\n",
    "    for j in train_imgs[25:45]:\n",
    "        imgs.append(j)\n",
    "    for j in train_imgs[-45:-25]:\n",
    "        imgs.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "for train_folder in train_folders :\n",
    "    json_path = glob(train_folder + '/*.json')[0]\n",
    "    js = json.load(open(json_path))\n",
    "    cat = js.get('action')\n",
    "    answers.append([train_folder.replace(data_path,''),cat])\n",
    "\n",
    "answers = pd.DataFrame(answers, columns = ['train_path','answer'])\n",
    "answers\n",
    "answers[\"answer\"][33] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:04<00:00, 30.35it/s]\n"
     ]
    }
   ],
   "source": [
    "labels = np.zeros(len(imgs))\n",
    "for i in tqdm(range(answers.shape[0])):\n",
    "    for j in range(len(imgs)):\n",
    "        if answers[\"train_path\"][i] in imgs[j]:\n",
    "            labels[j] = answers[\"answer\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "dropout_rate = 0.1\n",
    "class_num = 6\n",
    "learning_rate = 1e-4\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 25\n",
    "MODELS = 'efficientnet-b0'\n",
    "save_path = f\"./models/Final_{EPOCHS}\"\n",
    "FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.array(imgs)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "imgs, labels = shuffle(imgs, labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumentations_transform = A.Compose([\n",
    "    A.RandomGamma(),\n",
    "    A.ShiftScaleRotate(),\n",
    "    A.GaussianBlur(),\n",
    "    A.GaussNoise()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, imgs, labels=None, transformer=None, mode=\"train\"):\n",
    "        self.imgs = imgs\n",
    "        self.transformer = transformer\n",
    "        self.mode = mode\n",
    "        if self.mode == \"train\" :\n",
    "            self.labels = labels\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        img = cv2.imread(self.imgs[i]).astype(np.float32)/255\n",
    "        img = cv2.resize(img, dsize=(224,224))\n",
    "        if self.mode == \"train\":\n",
    "            if self.transformer != None:\n",
    "                img = self.transformer(image=img)\n",
    "                img = np.transpose(img[\"image\"], (2,0,1))\n",
    "            else:\n",
    "                img = np.transpose(img, (2,0,1))\n",
    "            return {\n",
    "                \"img\" : torch.tensor(img, dtype=torch.float32),\n",
    "                \"label\" : torch.tensor(self.labels[i], dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            img = np.transpose(img, (2,0,1))\n",
    "            return {\n",
    "                \"img\" : torch.tensor(img, dtype=torch.float32)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch_item, epoch, batch, training):\n",
    "    img = batch_item['img'].to(device)\n",
    "    label = batch_item['label'].to(device)\n",
    "    if training is True:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(img)\n",
    "            loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(img)\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [00:31,  8.95it/s, Epoch=1, Loss=1.167573, Total Loss=1.496572]\n",
      "71it [00:02, 29.24it/s, Epoch=1, Val Loss=1.052727, Total Val Loss=1.105829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [00:32,  8.75it/s, Epoch=2, Loss=0.655527, Total Loss=1.063289]\n",
      "71it [00:02, 28.71it/s, Epoch=2, Val Loss=0.858720, Total Val Loss=0.919579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [00:32,  8.77it/s, Epoch=3, Loss=0.825107, Total Loss=0.988487]\n",
      "71it [00:02, 27.90it/s, Epoch=3, Val Loss=1.481458, Total Val Loss=0.896710]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [00:33,  8.58it/s, Epoch=4, Loss=1.154914, Total Loss=0.946190]\n",
      "71it [00:02, 24.63it/s, Epoch=4, Val Loss=0.734872, Total Val Loss=0.862482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [00:34,  8.30it/s, Epoch=5, Loss=0.950113, Total Loss=0.891849]\n",
      "71it [00:02, 24.87it/s, Epoch=5, Val Loss=0.663219, Total Val Loss=0.868961]\n",
      "284it [00:34,  8.21it/s, Epoch=6, Loss=0.681989, Total Loss=0.806364]\n",
      "71it [00:02, 24.76it/s, Epoch=6, Val Loss=0.694261, Total Val Loss=0.719491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [00:34,  8.28it/s, Epoch=7, Loss=0.256926, Total Loss=0.733518]\n",
      "71it [00:02, 25.34it/s, Epoch=7, Val Loss=0.839757, Total Val Loss=0.647697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [00:35,  7.99it/s, Epoch=8, Loss=0.694262, Total Loss=0.699280]\n",
      "71it [00:02, 24.37it/s, Epoch=8, Val Loss=0.545413, Total Val Loss=0.635735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [00:33,  8.44it/s, Epoch=9, Loss=0.521789, Total Loss=0.645714]\n",
      "71it [00:02, 24.74it/s, Epoch=9, Val Loss=0.457722, Total Val Loss=0.523832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [00:34,  8.15it/s, Epoch=10, Loss=0.265421, Total Loss=0.598353]\n",
      "71it [00:02, 24.45it/s, Epoch=10, Val Loss=0.492630, Total Val Loss=0.562009]\n",
      "284it [00:34,  8.14it/s, Epoch=11, Loss=0.130105, Total Loss=0.578625]\n",
      "71it [00:02, 24.28it/s, Epoch=11, Val Loss=0.286832, Total Val Loss=0.466145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [00:32,  8.80it/s, Epoch=12, Loss=0.649750, Total Loss=0.554821]\n",
      "71it [00:02, 28.50it/s, Epoch=12, Val Loss=0.627870, Total Val Loss=0.558915]\n",
      "284it [00:32,  8.84it/s, Epoch=13, Loss=0.602600, Total Loss=0.527562]\n",
      "71it [00:02, 29.46it/s, Epoch=13, Val Loss=0.629946, Total Val Loss=0.413143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [00:32,  8.80it/s, Epoch=14, Loss=0.560306, Total Loss=0.495230]\n",
      "71it [00:02, 26.57it/s, Epoch=14, Val Loss=0.531883, Total Val Loss=0.426137]\n",
      "284it [00:31,  8.98it/s, Epoch=15, Loss=0.221211, Total Loss=0.468851]\n",
      "71it [00:02, 29.19it/s, Epoch=15, Val Loss=0.665586, Total Val Loss=0.447493]\n",
      "284it [00:32,  8.81it/s, Epoch=16, Loss=0.395507, Total Loss=0.448970]\n",
      "71it [00:02, 29.15it/s, Epoch=16, Val Loss=0.271607, Total Val Loss=0.402512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [00:31,  8.90it/s, Epoch=17, Loss=0.251221, Total Loss=0.454178]\n",
      "71it [00:02, 29.73it/s, Epoch=17, Val Loss=0.147530, Total Val Loss=0.394197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [00:31,  8.93it/s, Epoch=18, Loss=0.309420, Total Loss=0.435367]\n",
      "71it [00:02, 28.80it/s, Epoch=18, Val Loss=0.357886, Total Val Loss=0.355474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [00:30,  9.37it/s, Epoch=19, Loss=0.066127, Total Loss=0.424044]\n",
      "71it [00:02, 28.31it/s, Epoch=19, Val Loss=0.468782, Total Val Loss=0.375409]\n",
      "284it [00:31,  9.03it/s, Epoch=20, Loss=0.777823, Total Loss=0.423422]\n",
      "71it [00:02, 28.15it/s, Epoch=20, Val Loss=0.547918, Total Val Loss=0.349284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [00:28,  9.83it/s, Epoch=21, Loss=0.354864, Total Loss=0.403497]\n",
      "71it [00:02, 23.76it/s, Epoch=21, Val Loss=0.536819, Total Val Loss=0.346329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [00:34,  8.17it/s, Epoch=22, Loss=0.273219, Total Loss=0.377329]\n",
      "71it [00:02, 24.15it/s, Epoch=22, Val Loss=0.499153, Total Val Loss=0.303536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Model Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [00:34,  8.19it/s, Epoch=23, Loss=0.228693, Total Loss=0.386759]\n",
      "71it [00:02, 24.44it/s, Epoch=23, Val Loss=0.260221, Total Val Loss=0.325302]\n",
      "284it [00:34,  8.30it/s, Epoch=24, Loss=0.465904, Total Loss=0.369123]\n",
      "71it [00:02, 24.20it/s, Epoch=24, Val Loss=0.368085, Total Val Loss=0.323131]\n",
      "284it [00:35,  8.08it/s, Epoch=25, Loss=0.540739, Total Loss=0.346619]\n",
      "71it [00:03, 23.53it/s, Epoch=25, Val Loss=0.138858, Total Val Loss=0.305045]\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=FOLDS, random_state=42, shuffle=True)\n",
    "\n",
    "n_iter = 0\n",
    "\n",
    "for train_idx, val_idx in skf.split(imgs, labels):\n",
    "    \n",
    "    model = EfficientNet.from_pretrained(MODELS, num_classes=class_num, advprop=True)\n",
    "    model._dropout.p = dropout_rate\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "    \n",
    "    n_iter += 1\n",
    "\n",
    "    train_dataset = CustomDataset(imgs[train_idx], labels[train_idx], transformer=albumentations_transform)\n",
    "    val_dataset = CustomDataset(imgs[val_idx], labels[val_idx], transformer=albumentations_transform)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=16, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=16, shuffle=True)\n",
    "\n",
    "    sample_batch = next(iter(train_dataloader))\n",
    "\n",
    "\n",
    "    loss_plot, val_loss_plot = [], []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss, total_val_loss = 0, 0\n",
    "\n",
    "        tqdm_dataset = tqdm(enumerate(train_dataloader))\n",
    "        training = True\n",
    "        for batch, batch_item in tqdm_dataset:\n",
    "            batch_loss = train_step(batch_item, epoch, batch, training)\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            tqdm_dataset.set_postfix({\n",
    "                'Epoch': epoch + 1,\n",
    "                'Loss': '{:06f}'.format(batch_loss.item()),\n",
    "                'Total Loss' : '{:06f}'.format(total_loss/(batch+1))\n",
    "            })\n",
    "        loss_plot.append(total_loss/(batch+1))\n",
    "\n",
    "        tqdm_dataset = tqdm(enumerate(val_dataloader))\n",
    "        training = False\n",
    "        for batch, batch_item in tqdm_dataset:\n",
    "            batch_loss = train_step(batch_item, epoch, batch, training)\n",
    "            total_val_loss += batch_loss\n",
    "\n",
    "            tqdm_dataset.set_postfix({\n",
    "                'Epoch': epoch + 1,\n",
    "                'Val Loss': '{:06f}'.format(batch_loss.item()),\n",
    "                'Total Val Loss' : '{:06f}'.format(total_val_loss/(batch+1))\n",
    "            })\n",
    "        val_loss_plot.append(total_val_loss/(batch+1))\n",
    "        scheduler.step(total_val_loss/(batch+1))\n",
    "\n",
    "        if np.min(val_loss_plot) == val_loss_plot[-1]:\n",
    "            torch.save(model, save_path + f\"_{n_iter}.pt\")\n",
    "            print(\"## Model Save\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import albumentations as A\n",
    "import timm\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter('ignore')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = 'efficientnet-b0'\n",
    "save_path = f\"./models/Final\"\n",
    "device = torch.device(\"cuda:0\")\n",
    "class_num = 6\n",
    "FOLDS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/data/competition/Dacon/Traffic_Hand_Signal/dataset'\n",
    "\n",
    "test_path = data_path + '/test'\n",
    "\n",
    "test_img_path = data_path + '/new_images/test'\n",
    "\n",
    "hand_gesture = pd.read_csv(data_path + '/action_information.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folders  = sorted(glob(test_path + '//*'), key = lambda x : int(x.split('/file_')[-1]))\n",
    "\n",
    "test_img_folders  = sorted(glob(test_img_path + '//*'), key = lambda x : int(x.split('/file_')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "\n",
    "for i in test_img_folders:\n",
    "    test_imgs = sorted(glob(i+\"/*.jpg\"), key = lambda x : int(x.split('.jpg')[0].split(\"/\")[-1]))\n",
    "    for j in test_imgs[30:45]:\n",
    "        imgs.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet.from_pretrained(MODELS, num_classes=class_num, advprop=True)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    for batch_item in dataset:\n",
    "        img = batch_item['img'].to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(img)\n",
    "        output = output.cpu().numpy()\n",
    "        result.extend(output)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    f_x = np.exp(x) / np.sum(np.exp(x))\n",
    "    return f_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/test/file_142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/test/file_143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/test/file_144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/test/file_145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/test/file_146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/test/file_147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/test/file_148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/test/file_149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/test/file_150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/test/file_151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/test/file_152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/test/file_153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/test/file_154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/test/file_155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/test/file_156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>/test/file_157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>/test/file_158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>/test/file_159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>/test/file_160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>/test/file_161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>/test/file_162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>/test/file_163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>/test/file_164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>/test/file_165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>/test/file_166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>/test/file_167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>/test/file_168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>/test/file_169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>/test/file_170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>/test/file_171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>/test/file_172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>/test/file_173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>/test/file_174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>/test/file_175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>/test/file_176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>/test/file_177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>/test/file_178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>/test/file_179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>/test/file_180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>/test/file_181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>/test/file_182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>/test/file_183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>/test/file_184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>/test/file_185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>/test/file_186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         test_path\n",
       "0   /test/file_142\n",
       "1   /test/file_143\n",
       "2   /test/file_144\n",
       "3   /test/file_145\n",
       "4   /test/file_146\n",
       "5   /test/file_147\n",
       "6   /test/file_148\n",
       "7   /test/file_149\n",
       "8   /test/file_150\n",
       "9   /test/file_151\n",
       "10  /test/file_152\n",
       "11  /test/file_153\n",
       "12  /test/file_154\n",
       "13  /test/file_155\n",
       "14  /test/file_156\n",
       "15  /test/file_157\n",
       "16  /test/file_158\n",
       "17  /test/file_159\n",
       "18  /test/file_160\n",
       "19  /test/file_161\n",
       "20  /test/file_162\n",
       "21  /test/file_163\n",
       "22  /test/file_164\n",
       "23  /test/file_165\n",
       "24  /test/file_166\n",
       "25  /test/file_167\n",
       "26  /test/file_168\n",
       "27  /test/file_169\n",
       "28  /test/file_170\n",
       "29  /test/file_171\n",
       "30  /test/file_172\n",
       "31  /test/file_173\n",
       "32  /test/file_174\n",
       "33  /test/file_175\n",
       "34  /test/file_176\n",
       "35  /test/file_177\n",
       "36  /test/file_178\n",
       "37  /test/file_179\n",
       "38  /test/file_180\n",
       "39  /test/file_181\n",
       "40  /test/file_182\n",
       "41  /test/file_183\n",
       "42  /test/file_184\n",
       "43  /test/file_185\n",
       "44  /test/file_186"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = []\n",
    "for test_folder in test_folders :\n",
    "    json_path = glob(test_folder + '/*.json')[0]\n",
    "    js = json.load(open(json_path))\n",
    "    answers.append([test_folder.replace(data_path,'')])\n",
    "\n",
    "answers = pd.DataFrame(answers, columns = ['test_path'])\n",
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.61s/it]\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv(data_path + '/sample_submission.csv')\n",
    "sub_ALL = []\n",
    "sub_ALL_list = []\n",
    "\n",
    "for i in tqdm(range(FOLDS)):\n",
    "\n",
    "    model = torch.load(save_path + f\"_{EPOCHS}_1.pt\", map_location=device)\n",
    "\n",
    "    test_dataset = CustomDataset(imgs=imgs, labels=None, mode='test')\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=16)\n",
    "\n",
    "    pred = predict(test_dataloader)\n",
    "\n",
    "    a = []\n",
    "    for i in range(len(pred)):\n",
    "        a.append(softmax(pred[i]))\n",
    "    a = np.array(a)\n",
    "    \n",
    "    sub_ALL = []\n",
    "    \n",
    "    for i in range(answers.shape[0]):\n",
    "        sub = 0\n",
    "        num_sub = 0\n",
    "        for j in range(len(imgs)):\n",
    "            if answers[\"test_path\"][i] in imgs[j]:\n",
    "                num_sub = num_sub + 1\n",
    "                sub = sub + a[j]\n",
    "        sub = sub / num_sub\n",
    "        sub_ALL.append(sub)\n",
    "        \n",
    "    sub_ALL_list.append(sub_ALL)\n",
    "    \n",
    "sub_ALL_list = np.array(sub_ALL_list)\n",
    "sub_ALL_list = np.mean(sub_ALL_list, axis=0)\n",
    "submission.iloc[:,1:] = sub_ALL_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_CSV_NAME = f'./submission/Final.csv'\n",
    "\n",
    "submission.to_csv(SAVE_CSV_NAME, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
